{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e4a4fe1-2fa3-4eaf-bbf8-4ea77a6b47b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-06 21:05:57.640369: I external/xla/xla/pjrt/pjrt_api.cc:100] GetPjrtApi was found for tpu at /home/mazumdera/.local/lib/python3.10/site-packages/libtpu/libtpu.so\n",
      "2023-12-06 21:05:57.640400: I external/xla/xla/pjrt/pjrt_api.cc:79] PJRT_Api is set for device type tpu\n",
      "2023-12-06 21:05:57.640432: I external/xla/xla/pjrt/pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.40. The framework PJRT API version is 0.40.\n",
      "2023-12-06 21:06:00.316908: I external/xla/xla/pjrt/pjrt_c_api_client.cc:134] PjRtCApiClient created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 devices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-06 21:06:03.281930: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-06 21:06:03.546683: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-06 21:06:03.546738: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-06 21:06:03.547933: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-06 21:06:03.638436: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-06 21:06:03.638945: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-06 21:06:04.642915: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/mazumdera/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Get all the imports\n",
    "import jax\n",
    "import os\n",
    "import sys\n",
    "\n",
    "jax.config.update('jax_default_prng_impl', 'unsafe_rbg')\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\n",
    "os.environ[\"LIBTPU_INIT_ARGS\"] = os.environ.get(\"LIBTPU_INIT_ARGS\",\"\") + \" --xla_tpu_spmd_rng_bit_generator_unsafe=true\"\n",
    "print(f\"Found {jax.device_count()} devices.\")\n",
    "\n",
    "from typing import Sequence\n",
    "import datetime\n",
    "from absl import app\n",
    "from flax.linen import partitioning as nn_partitioning\n",
    "from flax import linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from layers import Transformer\n",
    "import pyconfig\n",
    "from input_pipeline import create_data_iterator_with_tokenizer\n",
    "import max_utils\n",
    "import checkpointing\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from jax.sharding import PartitionSpec as P\n",
    "from jax.sharding import Mesh\n",
    "\n",
    "from jax.experimental.compilation_cache import compilation_cache as cc\n",
    "\n",
    "from cloud_tpu_diagnostics import diagnostic\n",
    "from cloud_tpu_diagnostics.configuration import debug_configuration\n",
    "from cloud_tpu_diagnostics.configuration import diagnostic_configuration\n",
    "from cloud_tpu_diagnostics.configuration import stack_trace_configuration\n",
    "\n",
    "import max_utils\n",
    "from jax.sharding import PartitionSpec as P\n",
    "import max_logging\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c58e9bca-42d0-4f90-844f-58221f35a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "140159fe-2749-4fb2-8b3b-b391f42cc23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_output_directory=\"base_output_directory=gs://mazumdera-test-bucket/maxtext/llama2/12062023/1\"\n",
    "base_num_decoder_layers=\"base_num_decoder_layers=32\"\n",
    "base_num_heads = \"base_num_heads=32\"\n",
    "head_nums = \"head_dim=128\"\n",
    "# activation_function=\"\\\"relu\\\"\"\n",
    "# mlp_activations = f\"mlp_activations=[{activation_function}]\"\n",
    "async_checkpointing=\"async_checkpointing=False\" \n",
    "enable_dropout=\"enable_dropout=False\"\n",
    "\n",
    "#ici_data_parallelism=1\n",
    "#ici_fsdp_parallelism=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e9142b6-1553-474d-a83b-e4774d9da217",
   "metadata": {},
   "outputs": [],
   "source": [
    "commandline_args = [\"dummy\", \"/home/mazumdera/maxtext/MaxText/configs/base.yml\",\"run_name=1xv4-8\", \"dcn_data_parallelism=1\", \"save_period=5\",\"ici_data_parallelism=1\",\"ici_tensor_parallelism=1\",\"steps=20\",\"enable_profiler=true\",\"remat_policy=full\",\"base_emb_dim=512\", base_num_heads, head_nums,\"vocab_size=50272\", base_num_decoder_layers, \"per_device_batch_size=0.5\",\"enable_profiler=true\", \"base_mlp_dim=2048\", base_output_directory, \"max_predict_length=512\", async_checkpointing, enable_dropout]# , mlp_activations]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "072a3b77-b2a3-4c58-9ac4-9010bc12f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyconfig.initialize(commandline_args)\n",
    "config = pyconfig.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee6f0e1c-0017-4f69-9422-89d36d70629f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating checkpoint manager...\n",
      "Checkpoint manager created!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_manager = checkpointing.create_orbax_checkpoint_manager(\n",
    "      config.checkpoint_dir,\n",
    "      config.enable_checkpointing,\n",
    "      config.async_checkpointing,\n",
    "      config.save_period,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a479045-de7e-4a78-8b3a-d2838fdcb6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial PRNG Keys\n",
    "init_rng, nextrng = random.split(random.PRNGKey(config.init_weights_seed), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eb0ed19-6dbd-4107-bc0b-8b88890ed827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)] (num_devices: 4)\n",
      "Decided on mesh: [[[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]]\n",
      "\n",
      "  [[TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)]]\n",
      "\n",
      "  [[TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)]]\n",
      "\n",
      "  [[TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]]]]\n"
     ]
    }
   ],
   "source": [
    "devices_array = max_utils.create_device_mesh(config)\n",
    "mesh = Mesh(devices_array, config.mesh_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "318282b4-6d67-4b76-98d8-a3adbcb7b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(config, mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b9cff44-16b5-4401-abeb-83b2357ae07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x7fbe54e2dbd0>, update=<function chain.<locals>.update_fn at 0x7fbe54e2de10>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx = optax.adamw(\n",
    "      max_utils.create_learning_rate_schedule(config),\n",
    "      b1=config.adam_b1,\n",
    "      b2=config.adam_b2,\n",
    "      eps=config.adam_eps,\n",
    "      eps_root=config.adam_eps_root,\n",
    "      weight_decay=config.adam_weight_decay,\n",
    "  )\n",
    "tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18cf8659-8111-419d-8b08-c3915054a1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import max_utils\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a0400e3-83ed-4edb-9112-129cb5c9236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_train_state_partial = functools.partial(max_utils.init_train_state, model, tx,\n",
    "                                               config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4dd787e-17c3-4b9c-8755-76e0ee389796",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_state = jax.eval_shape(init_train_state_partial, init_rng)\n",
    "state_logical_annotations = nn.get_partition_spec(abstract_state)\n",
    "unboxed_abstract_state = max_utils.unbox_logicallypartioned_trainstate(abstract_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a51a6728-5ea3-4496-a76f-ec22900bca8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainState(step=ShapeDtypeStruct(shape=(), dtype=int32), apply_fn=<bound method Module.apply of Transformer(\n",
       "    # attributes\n",
       "    config = <pyconfig.HyperParameters object at 0x7fd17598c6d0>\n",
       "    mesh = Mesh(device_ids=array([[[[0]],\n",
       "    \n",
       "            [[2]],\n",
       "    \n",
       "            [[1]],\n",
       "    \n",
       "            [[3]]]]), axis_names=('data', 'fsdp', 'sequence', 'tensor'))\n",
       ")>, params={'decoder': {'decoder': {'mlp': {'ffn_layer1': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512, 32, 2048), dtype=float32), names=('embed', 'layers', 'mlp'), mesh=None, rules=None)}, 'wi': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512, 32, 2048), dtype=float32), names=('embed', 'layers', 'mlp'), mesh=None, rules=None)}, 'wo': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(2048, 32, 512), dtype=float32), names=('mlp', 'layers', 'embed'), mesh=None, rules=None)}}, 'post_self_attention_layer_norm': {'scale': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512, 32), dtype=float32), names=('embed', 'layers'), mesh=None, rules=None)}, 'pre_self_attention_layer_norm': {'scale': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512, 32), dtype=float32), names=('embed', 'layers'), mesh=None, rules=None)}, 'self_attention': {'key': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512, 32, 32, 128), dtype=float32), names=('embed', 'layers', 'heads', 'kv'), mesh=None, rules=None)}, 'out': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(32, 32, 128, 512), dtype=float32), names=('heads', 'layers', 'kv', 'embed'), mesh=None, rules=None)}, 'query': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512, 32, 32, 128), dtype=float32), names=('embed', 'layers', 'heads', 'kv'), mesh=None, rules=None)}, 'value': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512, 32, 32, 128), dtype=float32), names=('embed', 'layers', 'heads', 'kv'), mesh=None, rules=None)}}}, 'decoder_norm': {'scale': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512,), dtype=float32), names=('embed',), mesh=None, rules=None)}}, 'token_embedder': {'embedding': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(50272, 512), dtype=float32), names=('vocab', 'embed'), mesh=None, rules=None)}}, tx=GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x7fbe54e2dbd0>, update=<function chain.<locals>.update_fn at 0x7fbe54e2de10>), opt_state=(ScaleByAdamState(count=ShapeDtypeStruct(shape=(), dtype=int32), mu={'decoder': {'decoder': {'mlp': {'ffn_layer1': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512, 32, 2048), dtype=float32), names=('embed', 'layers', 'mlp'), mesh=None, rules=None)}, 'wi': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512, 32, 2048), dtype=float32), names=('embed', 'layers', 'mlp'), mesh=None, rules=None)}, 'wo': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(2048, 32, 512), dtype=float32), names=('mlp', 'layers', 'embed'), mesh=None, rules=None)}}, 'post_self_attention_layer_norm': {'scale': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512, 32), dtype=float32), names=('embed', 'layers'), mesh=None, rules=None)}, 'pre_self_attention_layer_norm': {'scale': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512, 32), dtype=float32), names=('embed', 'layers'), mesh=None, rules=None)}, 'self_attention': {'key': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512, 32, 32, 128), dtype=float32), names=('embed', 'layers', 'heads', 'kv'), mesh=None, rules=None)}, 'out': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(32, 32, 128, 512), dtype=float32), names=('heads', 'layers', 'kv', 'embed'), mesh=None, rules=None)}, 'query': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512, 32, 32, 128), dtype=float32), names=('embed', 'layers', 'heads', 'kv'), mesh=None, rules=None)}, 'value': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512, 32, 32, 128), dtype=float32), names=('embed', 'layers', 'heads', 'kv'), mesh=None, rules=None)}}}, 'decoder_norm': {'scale': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512,), dtype=float32), names=('embed',), mesh=None, rules=None)}}, 'token_embedder': {'embedding': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(50272, 512), dtype=float32), names=('vocab', 'embed'), mesh=None, rules=None)}}, nu={'decoder': {'decoder': {'mlp': {'ffn_layer1': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512, 32, 2048), dtype=float32), names=('embed', 'layers', 'mlp'), mesh=None, rules=None)}, 'wi': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512, 32, 2048), dtype=float32), names=('embed', 'layers', 'mlp'), mesh=None, rules=None)}, 'wo': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(2048, 32, 512), dtype=float32), names=('mlp', 'layers', 'embed'), mesh=None, rules=None)}}, 'post_self_attention_layer_norm': {'scale': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512, 32), dtype=float32), names=('embed', 'layers'), mesh=None, rules=None)}, 'pre_self_attention_layer_norm': {'scale': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512, 32), dtype=float32), names=('embed', 'layers'), mesh=None, rules=None)}, 'self_attention': {'key': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512, 32, 32, 128), dtype=float32), names=('embed', 'layers', 'heads', 'kv'), mesh=None, rules=None)}, 'out': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(32, 32, 128, 512), dtype=float32), names=('heads', 'layers', 'kv', 'embed'), mesh=None, rules=None)}, 'query': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512, 32, 32, 128), dtype=float32), names=('embed', 'layers', 'heads', 'kv'), mesh=None, rules=None)}, 'value': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512, 32, 32, 128), dtype=float32), names=('embed', 'layers', 'heads', 'kv'), mesh=None, rules=None)}}}, 'decoder_norm': {'scale': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(512,), dtype=float32), names=('embed',), mesh=None, rules=None)}}, 'token_embedder': {'embedding': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(50272, 512), dtype=float32), names=('vocab', 'embed'), mesh=None, rules=None)}}), EmptyState(), ScaleByScheduleState(count=ShapeDtypeStruct(shape=(), dtype=int32))))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b94af29-6191-413e-9bdd-f244200b7a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainState(step=PartitionSpec(), apply_fn=<bound method Module.apply of Transformer(\n",
       "    # attributes\n",
       "    config = <pyconfig.HyperParameters object at 0x7fd17598c6d0>\n",
       "    mesh = Mesh(device_ids=array([[[[0]],\n",
       "    \n",
       "            [[2]],\n",
       "    \n",
       "            [[1]],\n",
       "    \n",
       "            [[3]]]]), axis_names=('data', 'fsdp', 'sequence', 'tensor'))\n",
       ")>, params={'decoder': {'decoder': {'mlp': {'ffn_layer1': {'kernel': PartitionSpec('embed', 'layers', 'mlp')}, 'wi': {'kernel': PartitionSpec('embed', 'layers', 'mlp')}, 'wo': {'kernel': PartitionSpec('mlp', 'layers', 'embed')}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('embed', 'layers')}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('embed', 'layers')}, 'self_attention': {'key': {'kernel': PartitionSpec('embed', 'layers', 'heads', 'kv')}, 'out': {'kernel': PartitionSpec('heads', 'layers', 'kv', 'embed')}, 'query': {'kernel': PartitionSpec('embed', 'layers', 'heads', 'kv')}, 'value': {'kernel': PartitionSpec('embed', 'layers', 'heads', 'kv')}}}, 'decoder_norm': {'scale': PartitionSpec('embed',)}}, 'token_embedder': {'embedding': PartitionSpec('vocab', 'embed')}}, tx=GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x7fbe54e2dbd0>, update=<function chain.<locals>.update_fn at 0x7fbe54e2de10>), opt_state=(ScaleByAdamState(count=PartitionSpec(), mu={'decoder': {'decoder': {'mlp': {'ffn_layer1': {'kernel': PartitionSpec('embed', 'layers', 'mlp')}, 'wi': {'kernel': PartitionSpec('embed', 'layers', 'mlp')}, 'wo': {'kernel': PartitionSpec('mlp', 'layers', 'embed')}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('embed', 'layers')}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('embed', 'layers')}, 'self_attention': {'key': {'kernel': PartitionSpec('embed', 'layers', 'heads', 'kv')}, 'out': {'kernel': PartitionSpec('heads', 'layers', 'kv', 'embed')}, 'query': {'kernel': PartitionSpec('embed', 'layers', 'heads', 'kv')}, 'value': {'kernel': PartitionSpec('embed', 'layers', 'heads', 'kv')}}}, 'decoder_norm': {'scale': PartitionSpec('embed',)}}, 'token_embedder': {'embedding': PartitionSpec('vocab', 'embed')}}, nu={'decoder': {'decoder': {'mlp': {'ffn_layer1': {'kernel': PartitionSpec('embed', 'layers', 'mlp')}, 'wi': {'kernel': PartitionSpec('embed', 'layers', 'mlp')}, 'wo': {'kernel': PartitionSpec('mlp', 'layers', 'embed')}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('embed', 'layers')}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('embed', 'layers')}, 'self_attention': {'key': {'kernel': PartitionSpec('embed', 'layers', 'heads', 'kv')}, 'out': {'kernel': PartitionSpec('heads', 'layers', 'kv', 'embed')}, 'query': {'kernel': PartitionSpec('embed', 'layers', 'heads', 'kv')}, 'value': {'kernel': PartitionSpec('embed', 'layers', 'heads', 'kv')}}}, 'decoder_norm': {'scale': PartitionSpec('embed',)}}, 'token_embedder': {'embedding': PartitionSpec('vocab', 'embed')}}), EmptyState(), ScaleByScheduleState(count=PartitionSpec())))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_logical_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7fab72f-8de6-4d64-b9b5-110240fd3f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainState(step=ShapeDtypeStruct(shape=(), dtype=int32), apply_fn=<bound method Module.apply of Transformer(\n",
       "    # attributes\n",
       "    config = <pyconfig.HyperParameters object at 0x7fd17598c6d0>\n",
       "    mesh = Mesh(device_ids=array([[[[0]],\n",
       "    \n",
       "            [[2]],\n",
       "    \n",
       "            [[1]],\n",
       "    \n",
       "            [[3]]]]), axis_names=('data', 'fsdp', 'sequence', 'tensor'))\n",
       ")>, params={'decoder': {'decoder': {'mlp': {'ffn_layer1': {'kernel': ShapeDtypeStruct(shape=(512, 32, 2048), dtype=float32)}, 'wi': {'kernel': ShapeDtypeStruct(shape=(512, 32, 2048), dtype=float32)}, 'wo': {'kernel': ShapeDtypeStruct(shape=(2048, 32, 512), dtype=float32)}}, 'post_self_attention_layer_norm': {'scale': ShapeDtypeStruct(shape=(512, 32), dtype=float32)}, 'pre_self_attention_layer_norm': {'scale': ShapeDtypeStruct(shape=(512, 32), dtype=float32)}, 'self_attention': {'key': {'kernel': ShapeDtypeStruct(shape=(512, 32, 32, 128), dtype=float32)}, 'out': {'kernel': ShapeDtypeStruct(shape=(32, 32, 128, 512), dtype=float32)}, 'query': {'kernel': ShapeDtypeStruct(shape=(512, 32, 32, 128), dtype=float32)}, 'value': {'kernel': ShapeDtypeStruct(shape=(512, 32, 32, 128), dtype=float32)}}}, 'decoder_norm': {'scale': ShapeDtypeStruct(shape=(512,), dtype=float32)}}, 'token_embedder': {'embedding': ShapeDtypeStruct(shape=(50272, 512), dtype=float32)}}, tx=GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x7fbe54e2dbd0>, update=<function chain.<locals>.update_fn at 0x7fbe54e2de10>), opt_state=(ScaleByAdamState(count=ShapeDtypeStruct(shape=(), dtype=int32), mu={'decoder': {'decoder': {'mlp': {'ffn_layer1': {'kernel': ShapeDtypeStruct(shape=(512, 32, 2048), dtype=float32)}, 'wi': {'kernel': ShapeDtypeStruct(shape=(512, 32, 2048), dtype=float32)}, 'wo': {'kernel': ShapeDtypeStruct(shape=(2048, 32, 512), dtype=float32)}}, 'post_self_attention_layer_norm': {'scale': ShapeDtypeStruct(shape=(512, 32), dtype=float32)}, 'pre_self_attention_layer_norm': {'scale': ShapeDtypeStruct(shape=(512, 32), dtype=float32)}, 'self_attention': {'key': {'kernel': ShapeDtypeStruct(shape=(512, 32, 32, 128), dtype=float32)}, 'out': {'kernel': ShapeDtypeStruct(shape=(32, 32, 128, 512), dtype=float32)}, 'query': {'kernel': ShapeDtypeStruct(shape=(512, 32, 32, 128), dtype=float32)}, 'value': {'kernel': ShapeDtypeStruct(shape=(512, 32, 32, 128), dtype=float32)}}}, 'decoder_norm': {'scale': ShapeDtypeStruct(shape=(512,), dtype=float32)}}, 'token_embedder': {'embedding': ShapeDtypeStruct(shape=(50272, 512), dtype=float32)}}, nu={'decoder': {'decoder': {'mlp': {'ffn_layer1': {'kernel': ShapeDtypeStruct(shape=(512, 32, 2048), dtype=float32)}, 'wi': {'kernel': ShapeDtypeStruct(shape=(512, 32, 2048), dtype=float32)}, 'wo': {'kernel': ShapeDtypeStruct(shape=(2048, 32, 512), dtype=float32)}}, 'post_self_attention_layer_norm': {'scale': ShapeDtypeStruct(shape=(512, 32), dtype=float32)}, 'pre_self_attention_layer_norm': {'scale': ShapeDtypeStruct(shape=(512, 32), dtype=float32)}, 'self_attention': {'key': {'kernel': ShapeDtypeStruct(shape=(512, 32, 32, 128), dtype=float32)}, 'out': {'kernel': ShapeDtypeStruct(shape=(32, 32, 128, 512), dtype=float32)}, 'query': {'kernel': ShapeDtypeStruct(shape=(512, 32, 32, 128), dtype=float32)}, 'value': {'kernel': ShapeDtypeStruct(shape=(512, 32, 32, 128), dtype=float32)}}}, 'decoder_norm': {'scale': ShapeDtypeStruct(shape=(512,), dtype=float32)}}, 'token_embedder': {'embedding': ShapeDtypeStruct(shape=(50272, 512), dtype=float32)}}), EmptyState(), ScaleByScheduleState(count=ShapeDtypeStruct(shape=(), dtype=int32))))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unboxed_abstract_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aba1ef1-987a-4693-80ab-295b0c9a80ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to initialize backend 'tpu': ABORTED: The TPU is already in use by process with pid 129629. Not attempting to load libtpu.so in this process. (set JAX_PLATFORMS='' to automatically choose an available backend)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/xla_bridge.py:739\u001b[0m, in \u001b[0;36m_init_backend\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m    738\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing backend \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, platform)\n\u001b[0;32m--> 739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[43mregistration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;66;03m# TODO(skye): consider raising more descriptive errors directly from backend\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;66;03m# factories instead of returning None.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/xla_bridge.py:138\u001b[0m, in \u001b[0;36mtpu_client_timer_callback\u001b[0;34m(timer_secs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m   client \u001b[38;5;241m=\u001b[39m \u001b[43mxla_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_tpu_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_get_tpu_library_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jaxlib/xla_client.py:193\u001b[0m, in \u001b[0;36mmake_tpu_client\u001b[0;34m(library_path)\u001b[0m\n\u001b[1;32m    192\u001b[0m   profiler\u001b[38;5;241m.\u001b[39mregister_plugin_profiler(c_api)\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_tfrt_tpu_c_api_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jaxlib/xla_client.py:125\u001b[0m, in \u001b[0;36mmake_tfrt_tpu_c_api_client\u001b[0;34m(options)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pjrt_plugin_initialized(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtpu\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 125\u001b[0m   \u001b[43minitialize_pjrt_plugin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jaxlib/xla_client.py:162\u001b[0m, in \u001b[0;36minitialize_pjrt_plugin\u001b[0;34m(plugin_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initializes a PJRT plugin.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mThe plugin needs to be loaded first (through load_pjrt_plugin_dynamically or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m  plugin_name: the name of the PJRT plugin.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m \u001b[43m_xla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_pjrt_plugin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplugin_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: ABORTED: The TPU is already in use by process with pid 129629. Not attempting to load libtpu.so in this process.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Swap the first and second dimensions\u001b[39;00m\n\u001b[1;32m      6\u001b[0m swapped_a \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mtranspose(a, axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:2166\u001b[0m, in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m   2162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array(np\u001b[38;5;241m.\u001b[39masarray(view), dtype, copy, ndmin\u001b[38;5;241m=\u001b[39mndmin)\n\u001b[1;32m   2164\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected input type for array: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2166\u001b[0m out_array: Array \u001b[38;5;241m=\u001b[39m \u001b[43mlax_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_element_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweak_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweak_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ndmin \u001b[38;5;241m>\u001b[39m ndim(out_array):\n\u001b[1;32m   2169\u001b[0m   out_array \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mexpand_dims(out_array, \u001b[38;5;28mrange\u001b[39m(ndmin \u001b[38;5;241m-\u001b[39m ndim(out_array)))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/lax/lax.py:559\u001b[0m, in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type)\u001b[0m\n\u001b[1;32m    557\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m type_cast(Array, operand)\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 559\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_element_type_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mweak_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweak_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/core.py:402\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    400\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39menable_checks\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    401\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[0;32m--> 402\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_top_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/core.py:405\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 405\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/core.py:893\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 893\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/dispatch.py:87\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m     85\u001b[0m prev \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m   outs \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m   lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(prev)\n",
      "    \u001b[0;31m[... skipping hidden 16 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/xla_bridge.py:644\u001b[0m, in \u001b[0;36mbackends\u001b[0;34m()\u001b[0m\n\u001b[1;32m    642\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    643\u001b[0m         err_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 644\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(err_msg)\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m _default_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mjax_platforms\u001b[38;5;241m.\u001b[39mvalue:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to initialize backend 'tpu': ABORTED: The TPU is already in use by process with pid 129629. Not attempting to load libtpu.so in this process. (set JAX_PLATFORMS='' to automatically choose an available backend)"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "a = jnp.array([[1, 2,3,7], [4,5,6,8], [9,10,11,12], [13,14,15,16]])\n",
    "\n",
    "a = jnp.array.[[1, 2,3,7], [4,5,6,8], [9,10,11,12], [13,14,15,16]])\n",
    "\n",
    "# Swap the first and second dimensions\n",
    "swapped_a = jnp.transpose(a, axes=(1, 0,2,3))\n",
    "\n",
    "print(a.shape)\n",
    "print(swapped_a)  # Output: [[1 3], [2 4]]\n",
    "print(swapped_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d2f60-0555-4fa9-af4b-69c332d4e118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
