{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nb_Ppf2ZUQL"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AI-Hypercomputer/maxtext/blob/main/src/MaxText/examples/sft_qwen3_demo.ipynb)\n",
        "\n",
        "# Qwen3-0.6B Supervised Fine-Tuning (SFT) Demo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGbe4_YQZUQL"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook performs SFT training and evaluation workflow on [OpenAI's GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k).\n",
        "The primary goal is to demonstrate the end-to-end process of:\n",
        "1. Pre-SFT Evaluation: Calcuating baseline accuracy for the model before training.\n",
        "2. SFT Training: Fine-tune the model using MaxText & Tunix SFT trainer.\n",
        "3. Post-SFT Evaluation: Re-running the evaluation loop after training to measure the performance gain achieved by SFT.\n",
        "\n",
        "This notebook can run on the **public TPU v5e-1**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zolxPWhQZUQL"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "### Change Runtime Type\n",
        "\n",
        "**Instructions:**\n",
        "1.  Navigate to the menu at the top of the screen.\n",
        "2.  Click on **Runtime**.\n",
        "3.  Select **Change runtime type** from the dropdown menu.\n",
        "4.  Select **v5e-1 TPU** as the **Hardware accelerator**.\n",
        "5. Click on **Save**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk_QpVVuZUQL"
      },
      "source": [
        "### Get Your Hugging Face Token\n",
        "\n",
        "To access model checkpoint from the Hugging Face Hub, you need to authenticate with a personal access token.\n",
        "\n",
        "**Follow these steps to get your token:**\n",
        "\n",
        "1.  **Navigate to the Access Tokens page** in your Hugging Face account settings. You can go there directly by visiting this URL:\n",
        "    *   [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "\n",
        "2.  **Create a new token** by clicking the **\"+ Create new token\"** button.\n",
        "\n",
        "3.  **Give your token a name** and assign it a **`read` role**. The `read` role is sufficient for downloading models.\n",
        "\n",
        "4.  **Copy the generated token**. You will need to paste it in the next step.\n",
        "\n",
        "**Follow these steps to store your token:**\n",
        "\n",
        "1. On the left sidebar of your Colab window, click the key icon (the Secrets tab).\n",
        "\n",
        "2. Click **\"+ Add new secret\"**.\n",
        "\n",
        "3. Set the Name as **HF_TOKEN**.\n",
        "\n",
        "4. Paste your token into the Value field.\n",
        "\n",
        "5. Ensure the Notebook access toggle is turned On."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9ms-jTSZUQL"
      },
      "source": [
        "## Installation: MaxText & Other Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OSPRVbi7n6tB",
        "outputId": "5ad63081-bdf3-452d-ce03-1ec7571069a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'maxtext' already exists and is not an empty directory.\n",
            "/content/maxtext\n",
            "Requirement already satisfied: uv in /usr/local/lib/python3.12/dist-packages (0.9.10)\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m246 packages\u001b[0m \u001b[2min 1.06s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m5 packages\u001b[0m \u001b[2min 25ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m5 packages\u001b[0m \u001b[2min 12ms\u001b[0m\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mjax\u001b[0m\u001b[2m==0.7.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjax\u001b[0m\u001b[2m==0.8.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mjaxlib\u001b[0m\u001b[2m==0.7.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjaxlib\u001b[0m\u001b[2m==0.8.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mlibtpu\u001b[0m\u001b[2m==0.0.23\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlibtpu\u001b[0m\u001b[2m==0.0.24\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mmaxtext\u001b[0m\u001b[2m==0.1.1 (from file:///content/maxtext)\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==78.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==80.9.0\u001b[0m\n",
            "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe transitive dependency `google-cloud-aiplatform` is unpinned. Consider setting a lower bound with a constraint when using `--resolution lowest` to avoid using outdated versions.\u001b[0m\n",
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n",
            "2025-11-19 20:35:46.312104: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763584546.323919    7469 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763584546.327396    7469 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763584546.336575    7469 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763584546.336592    7469 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763584546.336595    7469 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763584546.336597    7469 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-19 20:35:51.158371: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "/usr/bin/python3: No module named MaxText.install_maxtext_extra_deps\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m195 packages\u001b[0m \u001b[2min 217ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m6 packages\u001b[0m \u001b[2min 30ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m6 packages\u001b[0m \u001b[2min 12ms\u001b[0m\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mflax\u001b[0m\u001b[2m==0.12.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mflax\u001b[0m\u001b[2m==0.11.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mjax\u001b[0m\u001b[2m==0.8.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjax\u001b[0m\u001b[2m==0.7.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mjaxlib\u001b[0m\u001b[2m==0.8.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjaxlib\u001b[0m\u001b[2m==0.7.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mlibtpu\u001b[0m\u001b[2m==0.0.24\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlibtpu\u001b[0m\u001b[2m==0.0.23\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mqwix\u001b[0m\u001b[2m==0.1.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mqwix\u001b[0m\u001b[2m==0.1.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==80.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==78.1.0\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AI-Hypercomputer/maxtext.git\n",
        "%cd /content/maxtext\n",
        "\n",
        "# Install uv, a fast Python package installer\n",
        "!pip install uv\n",
        "\n",
        "# Install MaxText and its dependencies\n",
        "!uv pip install -e .[tpu] --resolution=lowest\n",
        "!python3 -m MaxText.install_maxtext_extra_deps\n",
        "\n",
        "# Install vLLM for Jax and TPUs\n",
        "!uv pip install vllm-tpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywtealAxZUQM"
      },
      "source": [
        "### Restart Session\n",
        "To apply certain changes, you need to restart the session.\n",
        "\n",
        "**Instructions:**\n",
        "1.  Navigate to the menu at the top of the screen.\n",
        "2.  Click on **Runtime**.\n",
        "3.  Select **Restart session** from the dropdown menu.\n",
        "\n",
        "You will be asked to confirm the action in a pop-up dialog. Click on **Yes**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Clexf-j7ZUQM"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PkBI9A3JZUQM",
        "outputId": "4c64e1de-d8f2-4e39-9790-b14aee74b1f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MaxText installation path: /content/maxtext/src/MaxText\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import transformers\n",
        "\n",
        "import MaxText\n",
        "from MaxText import pyconfig\n",
        "from MaxText.examples.sft_train_and_evaluate import evaluate_model, get_test_dataset\n",
        "from MaxText.integration.tunix.tunix_adapter import TunixMaxTextAdapter\n",
        "from MaxText.sft import sft_trainer\n",
        "\n",
        "from tunix.rl.rollout.vllm_rollout import VllmRollout\n",
        "\n",
        "from datetime import datetime\n",
        "from flax import nnx\n",
        "from huggingface_hub import login\n",
        "\n",
        "try:\n",
        "  from google.colab import userdata\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "MAXTEXT_REPO_ROOT = os.path.dirname(MaxText.__file__)\n",
        "print(f\"MaxText installation path: {MAXTEXT_REPO_ROOT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBbPN-uVZUQM"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "except Exception:\n",
        "  HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
        "if HF_TOKEN:\n",
        "  login(token=HF_TOKEN)\n",
        "  print(\"Authenticated with Hugging Face successfully!\")\n",
        "else:\n",
        "  print(\"Authentication failed: HF_TOKEN is not set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aENuzm9iZUQM"
      },
      "source": [
        "## Model Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "RjPYYl3zZUQM"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"qwen3-0.6b\"\n",
        "TOKENIZER_PATH = \"Qwen/Qwen3-0.6B\"\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    TOKENIZER_PATH,\n",
        "    token=HF_TOKEN,\n",
        ")\n",
        "\n",
        "# This colab will download the checkpoint from HF and store at `MODEL_CHECKPOINT_PATH`\n",
        "MODEL_CHECKPOINT_PATH = f\"{MAXTEXT_REPO_ROOT}/qwen_checkpoint\"\n",
        "\n",
        "RUN_NAME = datetime.now().strftime(\"%Y-%m-%d-%H-%m-%S\")\n",
        "\n",
        "# This is the directory where the fine-tuned model checkpoint will be saved\n",
        "BASE_OUTPUT_DIRECTORY = f\"/tmp/maxtext_qwen06\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L37Ij4NZUQM"
      },
      "source": [
        "## Download Qwen3-0.6B Model Checkpoint from Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kJanDAc0ZUQM"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!python3 -m MaxText.utils.ckpt_conversion.to_maxtext \\\n",
        "    $MAXTEXT_REPO_ROOT/configs/base.yml \\\n",
        "    model_name=$MODEL_NAME \\\n",
        "    base_output_directory=$MODEL_CHECKPOINT_PATH \\\n",
        "    hf_access_token=$HF_TOKEN \\\n",
        "    use_multimodal=false \\\n",
        "    scan_layers=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1T0bm82lZUQM",
        "outputId": "0645b7dd-2bff-47d6-9fdd-6ed693a36318",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model checkpoint can be found at: /content/maxtext/src/MaxText/qwen_checkpoint/0/items\n"
          ]
        }
      ],
      "source": [
        "print(f\"Model checkpoint can be found at: {MODEL_CHECKPOINT_PATH}/0/items\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC-hILG0ZUQM"
      },
      "source": [
        "## Dataset Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "O3MLdr9kZUQM"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = \"openai/gsm8k\"\n",
        "TRAIN_DATA_SPLIT = \"train\"\n",
        "TEST_DATA_SPLIT = \"test\"\n",
        "HF_DATA_DIR = \"main\"\n",
        "TRAIN_DATA_COLUMNS = [\"question\", \"answer\"]\n",
        "CHAT_TEMPLATE_PATH = f\"{MAXTEXT_REPO_ROOT}/examples/chat_templates/math_qa.json\"\n",
        "NUM_TEST_SAMPLES = 20  # Total number of samples to test\n",
        "BATCH_SIZE = 1  # Number of test samples to process in a batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeAHmxSYZUQM"
      },
      "source": [
        "## MaxText Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "In-jdp1AAwrL",
        "outputId": "ad731ec9-00db-4067-a564-0fe5a8587601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:MaxText.pyconfig:Ignoring invalid/unsupported field from YAML/CLI: 'base_config'\n",
            "INFO:MaxText.pyconfig:Config param activations_in_float32: False\n",
            "INFO:MaxText.pyconfig:Config param adam_b1: 0.9\n",
            "INFO:MaxText.pyconfig:Config param adam_b2: 0.95\n",
            "INFO:MaxText.pyconfig:Config param adam_eps: 1e-08\n",
            "INFO:MaxText.pyconfig:Config param adam_eps_root: 0.0\n",
            "INFO:MaxText.pyconfig:Config param adam_weight_decay: 0.1\n",
            "INFO:MaxText.pyconfig:Config param add_bos: True\n",
            "INFO:MaxText.pyconfig:Config param add_eos: True\n",
            "INFO:MaxText.pyconfig:Config param allow_split_physical_axes: False\n",
            "INFO:MaxText.pyconfig:Config param ar_cache_axis_order: 1,2,0,3\n",
            "INFO:MaxText.pyconfig:Config param async_checkpointing: True\n",
            "INFO:MaxText.pyconfig:Config param attention: autoselected\n",
            "INFO:MaxText.pyconfig:Config param attention_bias: False\n",
            "INFO:MaxText.pyconfig:Config param attention_sink: False\n",
            "INFO:MaxText.pyconfig:Config param attention_type: global\n",
            "INFO:MaxText.pyconfig:Config param attn_logits_soft_cap: None\n",
            "INFO:MaxText.pyconfig:Config param autoregressive_decode_assert: \n",
            "INFO:MaxText.pyconfig:Config param base_emb_dim: 1024\n",
            "INFO:MaxText.pyconfig:Config param base_mlp_dim: 3072\n",
            "INFO:MaxText.pyconfig:Config param base_moe_mlp_dim: 7168\n",
            "INFO:MaxText.pyconfig:Config param base_num_decoder_layers: 28\n",
            "INFO:MaxText.pyconfig:Config param base_num_kv_heads: 8\n",
            "INFO:MaxText.pyconfig:Config param base_num_query_heads: 16\n",
            "INFO:MaxText.pyconfig:Config param base_output_directory: /tmp/maxtext_qwen06\n",
            "INFO:MaxText.pyconfig:Config param batch_size: 1\n",
            "INFO:MaxText.pyconfig:Config param beta_fast: 32\n",
            "INFO:MaxText.pyconfig:Config param beta_slow: 1\n",
            "INFO:MaxText.pyconfig:Config param capacity_factor: -1.0\n",
            "INFO:MaxText.pyconfig:Config param cast_logits_to_fp32: True\n",
            "INFO:MaxText.pyconfig:Config param chat_template_path: /content/maxtext/src/MaxText/examples/chat_templates/math_qa.json\n",
            "INFO:MaxText.pyconfig:Config param checkpoint_conversion_fn: None\n",
            "INFO:MaxText.pyconfig:Config param checkpoint_dir: /tmp/maxtext_qwen06/2025-11-19-20-11-00/checkpoints/\n",
            "INFO:MaxText.pyconfig:Config param checkpoint_is_quantized: False\n",
            "INFO:MaxText.pyconfig:Config param checkpoint_period: 10000\n",
            "INFO:MaxText.pyconfig:Config param checkpoint_storage_concurrent_gb: 96\n",
            "INFO:MaxText.pyconfig:Config param checkpoint_storage_target_data_file_size_bytes: 2147483648\n",
            "INFO:MaxText.pyconfig:Config param checkpoint_storage_use_ocdbt: True\n",
            "INFO:MaxText.pyconfig:Config param checkpoint_storage_use_zarr3: True\n",
            "INFO:MaxText.pyconfig:Config param chips_per_vm: 4\n",
            "INFO:MaxText.pyconfig:Config param chunk_attn_window_size: 0\n",
            "INFO:MaxText.pyconfig:Config param collect_stack_trace: False\n",
            "INFO:MaxText.pyconfig:Config param colocated_python_data_input: False\n",
            "INFO:MaxText.pyconfig:Config param compile_topology: \n",
            "INFO:MaxText.pyconfig:Config param compile_topology_num_slices: -1\n",
            "INFO:MaxText.pyconfig:Config param compiled_trainstep_file: \n",
            "INFO:MaxText.pyconfig:Config param compute_axis_order: 0,1,2,3\n",
            "INFO:MaxText.pyconfig:Config param constant_bound_config: []\n",
            "INFO:MaxText.pyconfig:Config param context: RematLocation.REMAT\n",
            "INFO:MaxText.pyconfig:Config param context_parallel_load_balance: True\n",
            "INFO:MaxText.pyconfig:Config param context_parallel_size: 1\n",
            "INFO:MaxText.pyconfig:Config param context_parallel_strategy: all_gather\n",
            "INFO:MaxText.pyconfig:Config param conv_stride_for_vit: 14\n",
            "INFO:MaxText.pyconfig:Config param cosine_learning_rate_final_fraction: 0.1\n",
            "INFO:MaxText.pyconfig:Config param cost_estimate_flops_bwd: -1\n",
            "INFO:MaxText.pyconfig:Config param cost_estimate_flops_fwd: -1\n",
            "INFO:MaxText.pyconfig:Config param custom_mesh: \n",
            "INFO:MaxText.pyconfig:Config param data_sharding: (('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'),)\n",
            "INFO:MaxText.pyconfig:Config param data_shuffle_seed: 0\n",
            "INFO:MaxText.pyconfig:Config param dataset_name: c4/en:3.0.1\n",
            "INFO:MaxText.pyconfig:Config param dataset_path: \n",
            "INFO:MaxText.pyconfig:Config param dataset_type: DatasetType.HF\n",
            "INFO:MaxText.pyconfig:Config param dcn_autoregressive_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param dcn_context_autoregressive_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param dcn_context_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param dcn_data_parallelism: -1\n",
            "INFO:MaxText.pyconfig:Config param dcn_expert_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param dcn_fsdp_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param dcn_fsdp_transpose_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param dcn_parallelism: [-1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "INFO:MaxText.pyconfig:Config param dcn_pipeline_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param dcn_sequence_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param dcn_tensor_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param dcn_tensor_sequence_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param dcn_tensor_transpose_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param debug: {'rl': False}\n",
            "INFO:MaxText.pyconfig:Config param decode_sampling_nucleus_p: -1\n",
            "INFO:MaxText.pyconfig:Config param decode_sampling_strategy: SamplingStrategy.GREEDY\n",
            "INFO:MaxText.pyconfig:Config param decode_sampling_temperature: 1.0\n",
            "INFO:MaxText.pyconfig:Config param decode_sampling_top_k: 0\n",
            "INFO:MaxText.pyconfig:Config param decoder_block: DecoderBlockType.QWEN3\n",
            "INFO:MaxText.pyconfig:Config param decoder_layer_input: RematLocation.DEVICE\n",
            "INFO:MaxText.pyconfig:Config param deepstack_visual_indexes_for_vit: []\n",
            "INFO:MaxText.pyconfig:Config param dpo_beta: 0.1\n",
            "INFO:MaxText.pyconfig:Config param dpo_label_smoothing: 0.0\n",
            "INFO:MaxText.pyconfig:Config param dq_reduction_steps: 0\n",
            "INFO:MaxText.pyconfig:Config param dropout_rate: 0.0\n",
            "INFO:MaxText.pyconfig:Config param dtype: bfloat16\n",
            "INFO:MaxText.pyconfig:Config param dtype_mm: float32\n",
            "INFO:MaxText.pyconfig:Config param dump_hlo: False\n",
            "INFO:MaxText.pyconfig:Config param dump_hlo_delete_local_after: True\n",
            "INFO:MaxText.pyconfig:Config param dump_hlo_gcs_dir: \n",
            "INFO:MaxText.pyconfig:Config param dump_hlo_local_dir: /tmp/xla_dump/\n",
            "INFO:MaxText.pyconfig:Config param dump_hlo_local_module_name: jit_train_step\n",
            "INFO:MaxText.pyconfig:Config param dump_hlo_module_name: jit_train_step\n",
            "INFO:MaxText.pyconfig:Config param dump_hlo_upload_all: False\n",
            "INFO:MaxText.pyconfig:Config param dump_hlo_xla_flags: \n",
            "INFO:MaxText.pyconfig:Config param dump_step: -1\n",
            "INFO:MaxText.pyconfig:Config param emb_dim: 1024\n",
            "INFO:MaxText.pyconfig:Config param enable_checkpoint_cloud_logger: False\n",
            "INFO:MaxText.pyconfig:Config param enable_checkpointing: True\n",
            "INFO:MaxText.pyconfig:Config param enable_data_shuffling: True\n",
            "INFO:MaxText.pyconfig:Config param enable_dropout: False\n",
            "INFO:MaxText.pyconfig:Config param enable_emergency_checkpoint: False\n",
            "INFO:MaxText.pyconfig:Config param enable_gcp_goodput_metrics: True\n",
            "INFO:MaxText.pyconfig:Config param enable_gcp_step_deviation_metrics: True\n",
            "INFO:MaxText.pyconfig:Config param enable_goodput_recording: False\n",
            "INFO:MaxText.pyconfig:Config param enable_jax_profiler: False\n",
            "INFO:MaxText.pyconfig:Config param enable_llm_inference_pool: False\n",
            "INFO:MaxText.pyconfig:Config param enable_model_warmup: False\n",
            "INFO:MaxText.pyconfig:Config param enable_multi_tier_checkpointing: False\n",
            "INFO:MaxText.pyconfig:Config param enable_nnx: False\n",
            "INFO:MaxText.pyconfig:Config param enable_orbax_v1: False\n",
            "INFO:MaxText.pyconfig:Config param enable_padding_causal_mask: True\n",
            "INFO:MaxText.pyconfig:Config param enable_pathways_goodput: False\n",
            "INFO:MaxText.pyconfig:Config param enable_prefix_caching: False\n",
            "INFO:MaxText.pyconfig:Config param enable_rampup_batch_size: False\n",
            "INFO:MaxText.pyconfig:Config param enable_single_controller: False\n",
            "INFO:MaxText.pyconfig:Config param enable_single_replica_ckpt_restoring: False\n",
            "INFO:MaxText.pyconfig:Config param enable_tensorboard: True\n",
            "INFO:MaxText.pyconfig:Config param enable_tunix_perf_metrics: False\n",
            "INFO:MaxText.pyconfig:Config param eval_corr_lst: False\n",
            "INFO:MaxText.pyconfig:Config param eval_data_columns: ['messages']\n",
            "INFO:MaxText.pyconfig:Config param eval_dataset_name: c4/en:3.0.1\n",
            "INFO:MaxText.pyconfig:Config param eval_image_column: image\n",
            "INFO:MaxText.pyconfig:Config param eval_interval: -1\n",
            "INFO:MaxText.pyconfig:Config param eval_make_lst: False\n",
            "INFO:MaxText.pyconfig:Config param eval_per_device_batch_size: 1\n",
            "INFO:MaxText.pyconfig:Config param eval_sampling_strategy: greedy\n",
            "INFO:MaxText.pyconfig:Config param eval_split: validation\n",
            "INFO:MaxText.pyconfig:Config param eval_steps: -1\n",
            "INFO:MaxText.pyconfig:Config param expansion_factor_real_data: -1.0\n",
            "INFO:MaxText.pyconfig:Config param expert_shard_attention_option: fsdp\n",
            "INFO:MaxText.pyconfig:Config param final_logits_soft_cap: None\n",
            "INFO:MaxText.pyconfig:Config param first_num_dense_layers: 0\n",
            "INFO:MaxText.pyconfig:Config param float32_logits: False\n",
            "INFO:MaxText.pyconfig:Config param float32_qk_product: False\n",
            "INFO:MaxText.pyconfig:Config param float32_weight_sum: True\n",
            "INFO:MaxText.pyconfig:Config param force_unroll: False\n",
            "INFO:MaxText.pyconfig:Config param freeze_vision_encoder_params: True\n",
            "INFO:MaxText.pyconfig:Config param fsdp_shard_on_exp: False\n",
            "INFO:MaxText.pyconfig:Config param fused_mlp: False\n",
            "INFO:MaxText.pyconfig:Config param fused_qkv: False\n",
            "INFO:MaxText.pyconfig:Config param gcs_metrics: False\n",
            "INFO:MaxText.pyconfig:Config param gdn_chunk_size: 64\n",
            "INFO:MaxText.pyconfig:Config param gdn_conv_kernel_dim: 4\n",
            "INFO:MaxText.pyconfig:Config param gdn_key_head_dim: 128\n",
            "INFO:MaxText.pyconfig:Config param gdn_num_key_heads: 16\n",
            "INFO:MaxText.pyconfig:Config param gdn_num_value_heads: 32\n",
            "INFO:MaxText.pyconfig:Config param gdn_value_head_dim: 128\n",
            "INFO:MaxText.pyconfig:Config param generate_padding_batch_eval: False\n",
            "INFO:MaxText.pyconfig:Config param generate_padding_batch_train: False\n",
            "INFO:MaxText.pyconfig:Config param generate_slice: v5e-16\n",
            "INFO:MaxText.pyconfig:Config param generation_configs: {}\n",
            "INFO:MaxText.pyconfig:Config param global_batch_size_to_eval_on: 1\n",
            "INFO:MaxText.pyconfig:Config param global_batch_size_to_load: 1\n",
            "INFO:MaxText.pyconfig:Config param global_batch_size_to_load_eval: 1\n",
            "INFO:MaxText.pyconfig:Config param global_batch_size_to_load_increment: None\n",
            "INFO:MaxText.pyconfig:Config param global_batch_size_to_load_start: None\n",
            "INFO:MaxText.pyconfig:Config param global_batch_size_to_train_on: 1\n",
            "INFO:MaxText.pyconfig:Config param global_parameter_scale: 1\n",
            "INFO:MaxText.pyconfig:Config param global_rampup_samples: 500\n",
            "INFO:MaxText.pyconfig:Config param goodput_upload_interval_seconds: 30\n",
            "INFO:MaxText.pyconfig:Config param grad_dtype: float32\n",
            "INFO:MaxText.pyconfig:Config param gradient_accumulation_steps: 1\n",
            "INFO:MaxText.pyconfig:Config param gradient_clipping_threshold: 1.0\n",
            "INFO:MaxText.pyconfig:Config param grain_eval_files: \n",
            "INFO:MaxText.pyconfig:Config param grain_file_type: arrayrecord\n",
            "INFO:MaxText.pyconfig:Config param grain_per_worker_buffer_size: 1\n",
            "INFO:MaxText.pyconfig:Config param grain_per_worker_buffer_size_eval: 1\n",
            "INFO:MaxText.pyconfig:Config param grain_train_files: \n",
            "INFO:MaxText.pyconfig:Config param grain_worker_count: 1\n",
            "INFO:MaxText.pyconfig:Config param grain_worker_count_eval: 1\n",
            "INFO:MaxText.pyconfig:Config param grpo_beta: 0.08\n",
            "INFO:MaxText.pyconfig:Config param grpo_epsilon: 0.2\n",
            "INFO:MaxText.pyconfig:Config param hardware: tpu\n",
            "INFO:MaxText.pyconfig:Config param hbm_utilization_vllm: 0.72\n",
            "INFO:MaxText.pyconfig:Config param head_dim: 128\n",
            "INFO:MaxText.pyconfig:Config param heartbeat_reporting_interval_in_seconds: 5\n",
            "INFO:MaxText.pyconfig:Config param hf_data_dir: main\n",
            "INFO:MaxText.pyconfig:Config param hf_eval_files: \n",
            "INFO:MaxText.pyconfig:Config param hf_eval_split: test_sft\n",
            "INFO:MaxText.pyconfig:Config param hf_path: openai/gsm8k\n",
            "INFO:MaxText.pyconfig:Config param hf_train_files: \n",
            "INFO:MaxText.pyconfig:Config param hidden_size_for_vit: 1408\n",
            "INFO:MaxText.pyconfig:Config param hide_profiler_step_metric: False\n",
            "INFO:MaxText.pyconfig:Config param ici_autoregressive_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param ici_context_autoregressive_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param ici_context_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param ici_data_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param ici_expert_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param ici_fsdp_parallelism: -1\n",
            "INFO:MaxText.pyconfig:Config param ici_fsdp_transpose_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param ici_parallelism: [1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "INFO:MaxText.pyconfig:Config param ici_pipeline_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param ici_sequence_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param ici_tensor_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param ici_tensor_sequence_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param ici_tensor_transpose_parallelism: 1\n",
            "INFO:MaxText.pyconfig:Config param image_path: \n",
            "INFO:MaxText.pyconfig:Config param image_placeholder: <|image|>\n",
            "INFO:MaxText.pyconfig:Config param image_size_for_vit: 896\n",
            "INFO:MaxText.pyconfig:Config param inference_benchmark_test: False\n",
            "INFO:MaxText.pyconfig:Config param inference_metadata_file: \n",
            "INFO:MaxText.pyconfig:Config param inference_microbenchmark_log_file_path: \n",
            "INFO:MaxText.pyconfig:Config param inference_microbenchmark_loop_iters: 10\n",
            "INFO:MaxText.pyconfig:Config param inference_microbenchmark_num_samples: [1, 2, 3, 4, 5]\n",
            "INFO:MaxText.pyconfig:Config param inference_microbenchmark_prefill_lengths: 64,128,256,512,1024\n",
            "INFO:MaxText.pyconfig:Config param inference_microbenchmark_stages: prefill,generate\n",
            "INFO:MaxText.pyconfig:Config param inference_server: MaxtextInterleavedServer\n",
            "INFO:MaxText.pyconfig:Config param inhomogeneous_layer_cycle_interval: 1\n",
            "INFO:MaxText.pyconfig:Config param init_weights_seed: 0\n",
            "INFO:MaxText.pyconfig:Config param input_data_sharding_logical_axes: ['activation_embed_and_logits_batch', 'activation_norm_length']\n",
            "INFO:MaxText.pyconfig:Config param interleave_moe_layer_step: 1\n",
            "INFO:MaxText.pyconfig:Config param intermediate_size_for_vit: 5632\n",
            "INFO:MaxText.pyconfig:Config param jax_cache_dir: ~/jax_cache\n",
            "INFO:MaxText.pyconfig:Config param jax_debug_log_modules: \n",
            "INFO:MaxText.pyconfig:Config param jax_distributed_initialization_timeout: 300\n",
            "INFO:MaxText.pyconfig:Config param jax_profiler_port: 9999\n",
            "INFO:MaxText.pyconfig:Config param key_proj: RematLocation.REMAT\n",
            "INFO:MaxText.pyconfig:Config param kv_cache_buffer: 256\n",
            "INFO:MaxText.pyconfig:Config param kv_lora_rank: 512\n",
            "INFO:MaxText.pyconfig:Config param kv_quant_axis: KvQuantAxis.HEADS_AND_DKV\n",
            "INFO:MaxText.pyconfig:Config param kv_quant_dtype: int8\n",
            "INFO:MaxText.pyconfig:Config param learning_rate: 3e-06\n",
            "INFO:MaxText.pyconfig:Config param learning_rate_schedule_steps: 500\n",
            "INFO:MaxText.pyconfig:Config param load_balance_loss_weight: 0.01\n",
            "INFO:MaxText.pyconfig:Config param load_from_prefill_dir: False\n",
            "INFO:MaxText.pyconfig:Config param load_full_state_path: \n",
            "INFO:MaxText.pyconfig:Config param load_parameters_path: /content/maxtext/src/MaxText/qwen_checkpoint/0/items\n",
            "INFO:MaxText.pyconfig:Config param local_checkpoint_directory: \n",
            "INFO:MaxText.pyconfig:Config param local_checkpoint_period: 0\n",
            "INFO:MaxText.pyconfig:Config param local_rope_max_timescale: -1\n",
            "INFO:MaxText.pyconfig:Config param log_config: True\n",
            "INFO:MaxText.pyconfig:Config param log_period: 100\n",
            "INFO:MaxText.pyconfig:Config param logical_axis_rules: (('activation_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_batch_no_exp', ('data', 'fsdp', 'fsdp_transpose')), ('activation_embed_and_logits_batch', ('data', 'stage', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_embed_and_logits_batch_sequence', ('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('activation_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence', 'autoregressive')), ('activation_kv_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_length', ('sequence', 'context', 'expert')), ('activation_length', ('context', 'expert')), ('activation_length_no_exp', ('sequence', 'context')), ('activation_length_no_exp', ('context',)), ('activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('activation_q_length', ('context', 'expert')), ('activation_q_length_no_exp', ('context',)), ('prefill_activation_length', ('sequence', 'context')), ('prefill_activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('activation_kv_length', ()), ('activation_embed', ('tensor', 'tensor_transpose')), ('activation_mlp', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_kv', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_prefill_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_batch_no_exp', ('data', 'fsdp', 'fsdp_transpose')), ('activation_kv_head_dim', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose')), ('activation_vocab', 'tensor_sequence'), ('activation_vocab', ('sequence', 'context')), ('activation_stage', 'stage'), ('activation_exp', ('expert',)), ('decode_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('decode_length', ('sequence',)), ('mlp', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), ('mlp_no_fsdp', ('tensor', 'tensor_sequence', 'autoregressive')), ('vocab', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('q_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('kv_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'context', 'expert')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'context')), ('embed_tensor_transpose', ('tensor_transpose',)), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'expert')), ('q_lora_up_proj', ()), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'expert')), ('kv_lora_up_proj', ()), ('norm', ('tensor', 'tensor_transpose')), ('layers', 'stage'), ('kv', ()), ('kv_head_dim', ()), ('cache_batch_prefill', ()), ('cache_batch', ()), ('cache_heads_none', ()), ('cache_heads', ('autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence')), ('cache_heads', ('autoregressive', 'tensor', 'tensor_sequence')), ('cache_kv', ()), ('cache_sequence', ()), ('exp', 'expert'), ('paged_kv_heads', ('tensor',)), ('num_pages', ()), ('tokens_per_page', ()), ('paged_kv_head_dim_size', ()), ('dense_layers', ()), ('moe_layers', ()))\n",
            "INFO:MaxText.pyconfig:Config param logits_dot_in_fp32: False\n",
            "INFO:MaxText.pyconfig:Config param logits_via_embedding: True\n",
            "INFO:MaxText.pyconfig:Config param lora_input_adapters_path: \n",
            "INFO:MaxText.pyconfig:Config param loss_algo: grpo\n",
            "INFO:MaxText.pyconfig:Config param matmul_precision: MatmulPrecision.DEFAULT\n",
            "INFO:MaxText.pyconfig:Config param max_checkify: False\n",
            "INFO:MaxText.pyconfig:Config param max_corpus_chars: 10000000\n",
            "INFO:MaxText.pyconfig:Config param max_num_checkpoints_to_keep: None\n",
            "INFO:MaxText.pyconfig:Config param max_num_images_per_example: -1\n",
            "INFO:MaxText.pyconfig:Config param max_position_embeddings: 163840\n",
            "INFO:MaxText.pyconfig:Config param max_prefill_predict_length: 64\n",
            "INFO:MaxText.pyconfig:Config param max_segments_per_seq: 32\n",
            "INFO:MaxText.pyconfig:Config param max_target_length: 1024\n",
            "INFO:MaxText.pyconfig:Config param megablox: True\n",
            "INFO:MaxText.pyconfig:Config param mesh_axes: ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive']\n",
            "INFO:MaxText.pyconfig:Config param metrics_dir: /tmp/maxtext_qwen06/2025-11-19-20-11-00/metrics/\n",
            "INFO:MaxText.pyconfig:Config param metrics_file: \n",
            "INFO:MaxText.pyconfig:Config param micro_batch_size: -1\n",
            "INFO:MaxText.pyconfig:Config param micro_batch_size_to_eval_on: 1\n",
            "INFO:MaxText.pyconfig:Config param micro_batch_size_to_train_on: 1\n",
            "INFO:MaxText.pyconfig:Config param mla_naive_kvcache: True\n",
            "INFO:MaxText.pyconfig:Config param mlp_activations: ['silu', 'linear']\n",
            "INFO:MaxText.pyconfig:Config param mlp_activations_limit: -1.0\n",
            "INFO:MaxText.pyconfig:Config param mlp_bias: False\n",
            "INFO:MaxText.pyconfig:Config param mlp_dim: 3072\n",
            "INFO:MaxText.pyconfig:Config param mlpwi: RematLocation.REMAT\n",
            "INFO:MaxText.pyconfig:Config param mlpwi_0: RematLocation.REMAT\n",
            "INFO:MaxText.pyconfig:Config param mlpwi_1: RematLocation.REMAT\n",
            "INFO:MaxText.pyconfig:Config param mlpwo: RematLocation.REMAT\n",
            "INFO:MaxText.pyconfig:Config param moba: False\n",
            "INFO:MaxText.pyconfig:Config param moba_chunk_size: 1024\n",
            "INFO:MaxText.pyconfig:Config param moba_topk: 8\n",
            "INFO:MaxText.pyconfig:Config param model_call_mode: \n",
            "INFO:MaxText.pyconfig:Config param model_fsdp_ag_once: False\n",
            "INFO:MaxText.pyconfig:Config param model_name: qwen3-0.6b\n",
            "INFO:MaxText.pyconfig:Config param moe_fsdp_use_two_stage_all_gather: False\n",
            "INFO:MaxText.pyconfig:Config param moe_mlp_dim: 7168\n",
            "INFO:MaxText.pyconfig:Config param monitor_goodput: False\n",
            "INFO:MaxText.pyconfig:Config param monitor_step_time_deviation: True\n",
            "INFO:MaxText.pyconfig:Config param mscale: 1.0\n",
            "INFO:MaxText.pyconfig:Config param mtc_data_parallelism: 0\n",
            "INFO:MaxText.pyconfig:Config param mtp_eval_target_module: 0\n",
            "INFO:MaxText.pyconfig:Config param mtp_loss_scaling_factor: 0.1\n",
            "INFO:MaxText.pyconfig:Config param mtp_num_layers: 0\n",
            "INFO:MaxText.pyconfig:Config param mu_dtype: bfloat16\n",
            "INFO:MaxText.pyconfig:Config param multi_sampling: False\n",
            "INFO:MaxText.pyconfig:Config param multi_tier_checkpointing_backup_interval_minutes: 0\n",
            "INFO:MaxText.pyconfig:Config param n_routing_groups: -1\n",
            "INFO:MaxText.pyconfig:Config param nope_layer_interval: -1\n",
            "INFO:MaxText.pyconfig:Config param norm_topk_prob: False\n",
            "INFO:MaxText.pyconfig:Config param normalization_layer_epsilon: 1e-06\n",
            "INFO:MaxText.pyconfig:Config param normalize_embedding_logits: False\n",
            "INFO:MaxText.pyconfig:Config param num_attention_heads_for_vit: 16\n",
            "INFO:MaxText.pyconfig:Config param num_batches: 4\n",
            "INFO:MaxText.pyconfig:Config param num_channels_for_vit: 3\n",
            "INFO:MaxText.pyconfig:Config param num_decoder_layers: 28\n",
            "INFO:MaxText.pyconfig:Config param num_epoch: 1\n",
            "INFO:MaxText.pyconfig:Config param num_eval_passes: 1\n",
            "INFO:MaxText.pyconfig:Config param num_experts: 1\n",
            "INFO:MaxText.pyconfig:Config param num_experts_per_tok: 1\n",
            "INFO:MaxText.pyconfig:Config param num_generations: 2\n",
            "INFO:MaxText.pyconfig:Config param num_hidden_layers_for_vit: 34\n",
            "INFO:MaxText.pyconfig:Config param num_iterations: 1\n",
            "INFO:MaxText.pyconfig:Config param num_kv_heads: 8\n",
            "INFO:MaxText.pyconfig:Config param num_layers_per_pipeline_stage: 1\n",
            "INFO:MaxText.pyconfig:Config param num_pipeline_microbatches: -1\n",
            "INFO:MaxText.pyconfig:Config param num_pipeline_repeats: -1\n",
            "INFO:MaxText.pyconfig:Config param num_position_embeddings_for_vit: 1024\n",
            "INFO:MaxText.pyconfig:Config param num_query_heads: 16\n",
            "INFO:MaxText.pyconfig:Config param num_samplers_slices: -1\n",
            "INFO:MaxText.pyconfig:Config param num_slices: 1\n",
            "INFO:MaxText.pyconfig:Config param num_test_batches: 5\n",
            "INFO:MaxText.pyconfig:Config param num_trainer_slices: -1\n",
            "INFO:MaxText.pyconfig:Config param num_vocab_tiling: 1\n",
            "INFO:MaxText.pyconfig:Config param opt_type: OptimizerType.ADAMW\n",
            "INFO:MaxText.pyconfig:Config param optimize_mesh_for_tpu_v6e: False\n",
            "INFO:MaxText.pyconfig:Config param optimizer_memory_host_offload: False\n",
            "INFO:MaxText.pyconfig:Config param original_max_position_embeddings: 4096\n",
            "INFO:MaxText.pyconfig:Config param out_hidden_size_for_vit: 512\n",
            "INFO:MaxText.pyconfig:Config param out_proj: RematLocation.REMAT\n",
            "INFO:MaxText.pyconfig:Config param override_model_config: False\n",
            "INFO:MaxText.pyconfig:Config param packing: True\n",
            "INFO:MaxText.pyconfig:Config param pagedattn_head_dim_alignment: 128\n",
            "INFO:MaxText.pyconfig:Config param pagedattn_max_pages_per_group: -1\n",
            "INFO:MaxText.pyconfig:Config param pagedattn_num_pages: 64\n",
            "INFO:MaxText.pyconfig:Config param pagedattn_pages_per_compute_block: 4\n",
            "INFO:MaxText.pyconfig:Config param pagedattn_tokens_per_page: 32\n",
            "INFO:MaxText.pyconfig:Config param param_scan_axis: 1\n",
            "INFO:MaxText.pyconfig:Config param parameter_memory_host_offload: False\n",
            "INFO:MaxText.pyconfig:Config param partial_rotary_factor: 1.0\n",
            "INFO:MaxText.pyconfig:Config param patch_size_for_vit: 14\n",
            "INFO:MaxText.pyconfig:Config param penalty_incorrect_answer: -1.0\n",
            "INFO:MaxText.pyconfig:Config param penalty_incorrect_format: -0.5\n",
            "INFO:MaxText.pyconfig:Config param per_device_batch_size: 1\n",
            "INFO:MaxText.pyconfig:Config param per_device_batch_size_increment: 2.0\n",
            "INFO:MaxText.pyconfig:Config param per_device_batch_size_start: 4.0\n",
            "INFO:MaxText.pyconfig:Config param pipeline_delay_activation_forwarding: False\n",
            "INFO:MaxText.pyconfig:Config param pipeline_fsdp_ag_once: False\n",
            "INFO:MaxText.pyconfig:Config param pipeline_parallel_layers: 28\n",
            "INFO:MaxText.pyconfig:Config param pixel_shuffle_ratio_for_vit: 0.5\n",
            "INFO:MaxText.pyconfig:Config param posemb_type_for_vit: learn\n",
            "INFO:MaxText.pyconfig:Config param prefill_cache_axis_order: 1,2,0,3\n",
            "INFO:MaxText.pyconfig:Config param prefill_cache_dir: \n",
            "INFO:MaxText.pyconfig:Config param prefill_chunk_size: 256\n",
            "INFO:MaxText.pyconfig:Config param prefill_slice: v5e-16\n",
            "INFO:MaxText.pyconfig:Config param prefix_caching_dram_byte: 100000000000\n",
            "INFO:MaxText.pyconfig:Config param prefix_caching_hbm_byte: 10000000000\n",
            "INFO:MaxText.pyconfig:Config param profile_cleanly: True\n",
            "INFO:MaxText.pyconfig:Config param profile_periodically_period: -1\n",
            "INFO:MaxText.pyconfig:Config param profiler: ProfilerType.NONE\n",
            "INFO:MaxText.pyconfig:Config param profiler_steps: 5\n",
            "INFO:MaxText.pyconfig:Config param projector_dropout_for_vit: 0.0\n",
            "INFO:MaxText.pyconfig:Config param projector_input_dim_for_vit: 4096\n",
            "INFO:MaxText.pyconfig:Config param projector_output_dim_for_vit: 4096\n",
            "INFO:MaxText.pyconfig:Config param prometheus_port: 0\n",
            "INFO:MaxText.pyconfig:Config param prompt: I love to\n",
            "INFO:MaxText.pyconfig:Config param q_lora_rank: 0\n",
            "INFO:MaxText.pyconfig:Config param qk_nope_head_dim: 128\n",
            "INFO:MaxText.pyconfig:Config param qk_rope_head_dim: 64\n",
            "INFO:MaxText.pyconfig:Config param qkv_proj: RematLocation.REMAT\n",
            "INFO:MaxText.pyconfig:Config param quant_cfg_path: \n",
            "INFO:MaxText.pyconfig:Config param quantization: QuantizationType.NONE\n",
            "INFO:MaxText.pyconfig:Config param quantization_calibration_method: absmax\n",
            "INFO:MaxText.pyconfig:Config param quantization_local_shard_count: 1\n",
            "INFO:MaxText.pyconfig:Config param quantize_kvcache: False\n",
            "INFO:MaxText.pyconfig:Config param query_proj: RematLocation.REMAT\n",
            "INFO:MaxText.pyconfig:Config param ragged_block_size: 256\n",
            "INFO:MaxText.pyconfig:Config param rampup_end_step: 0\n",
            "INFO:MaxText.pyconfig:Config param rampup_samples_per_increment_to_load: None\n",
            "INFO:MaxText.pyconfig:Config param reasoning_end_token: </reasoning>\n",
            "INFO:MaxText.pyconfig:Config param reasoning_start_token: <reasoning>\n",
            "INFO:MaxText.pyconfig:Config param record_internal_nn_metrics: 0\n",
            "INFO:MaxText.pyconfig:Config param remat_policy: full\n",
            "INFO:MaxText.pyconfig:Config param remat_policy_for_vit: minimal\n",
            "INFO:MaxText.pyconfig:Config param replicate_quant_scale: False\n",
            "INFO:MaxText.pyconfig:Config param replicator_backup_interval_minutes: 0\n",
            "INFO:MaxText.pyconfig:Config param report_heartbeat_metric_for_gcp_monitoring: False\n",
            "INFO:MaxText.pyconfig:Config param report_performance_metric_for_gcp_monitoring: False\n",
            "INFO:MaxText.pyconfig:Config param reshape_q: False\n",
            "INFO:MaxText.pyconfig:Config param return_log_prob: False\n",
            "INFO:MaxText.pyconfig:Config param reuse_example_batch: 0\n",
            "INFO:MaxText.pyconfig:Config param reward_exact_format_match: 3.0\n",
            "INFO:MaxText.pyconfig:Config param reward_partial_format_match: 0.5\n",
            "INFO:MaxText.pyconfig:Config param reward_ratio_guess_to_answer_high: 0.5\n",
            "INFO:MaxText.pyconfig:Config param reward_ratio_guess_to_answer_low: 0.25\n",
            "INFO:MaxText.pyconfig:Config param reward_white_space_format_match: 1.5\n",
            "INFO:MaxText.pyconfig:Config param rope_attention_scaling: False\n",
            "INFO:MaxText.pyconfig:Config param rope_factor: 40\n",
            "INFO:MaxText.pyconfig:Config param rope_interleave: True\n",
            "INFO:MaxText.pyconfig:Config param rope_linear_scaling_factor: 1.0\n",
            "INFO:MaxText.pyconfig:Config param rope_max_timescale: 1000000\n",
            "INFO:MaxText.pyconfig:Config param rope_min_timescale: 1\n",
            "INFO:MaxText.pyconfig:Config param rope_theta_for_vit: 10000\n",
            "INFO:MaxText.pyconfig:Config param rope_truncate: True\n",
            "INFO:MaxText.pyconfig:Config param rope_type: RopeType.DEFAULT\n",
            "INFO:MaxText.pyconfig:Config param rope_use_scale: True\n",
            "INFO:MaxText.pyconfig:Config param routed_bias: False\n",
            "INFO:MaxText.pyconfig:Config param routed_scaling_factor: 1.0\n",
            "INFO:MaxText.pyconfig:Config param routed_score_func: \n",
            "INFO:MaxText.pyconfig:Config param run_name: 2025-11-19-20-11-00\n",
            "INFO:MaxText.pyconfig:Config param sa_block_kv: 512\n",
            "INFO:MaxText.pyconfig:Config param sa_block_kv_compute: 512\n",
            "INFO:MaxText.pyconfig:Config param sa_block_kv_dkv: 512\n",
            "INFO:MaxText.pyconfig:Config param sa_block_kv_dkv_compute: 512\n",
            "INFO:MaxText.pyconfig:Config param sa_block_kv_dq: 512\n",
            "INFO:MaxText.pyconfig:Config param sa_block_q: 512\n",
            "INFO:MaxText.pyconfig:Config param sa_block_q_dkv: 512\n",
            "INFO:MaxText.pyconfig:Config param sa_block_q_dq: 512\n",
            "INFO:MaxText.pyconfig:Config param sa_k_layout: HEAD_DIM_MINOR\n",
            "INFO:MaxText.pyconfig:Config param sa_q_layout: HEAD_DIM_MINOR\n",
            "INFO:MaxText.pyconfig:Config param sa_use_fused_bwd_kernel: False\n",
            "INFO:MaxText.pyconfig:Config param sa_v_layout: HEAD_DIM_MINOR\n",
            "INFO:MaxText.pyconfig:Config param sampler_devices_fraction: 0.5\n",
            "INFO:MaxText.pyconfig:Config param save_checkpoint_on_completion: True\n",
            "INFO:MaxText.pyconfig:Config param save_config_to_gcs: False\n",
            "INFO:MaxText.pyconfig:Config param save_quantized_params_path: \n",
            "INFO:MaxText.pyconfig:Config param scan_layers: True\n",
            "INFO:MaxText.pyconfig:Config param scan_layers_per_stage: False\n",
            "INFO:MaxText.pyconfig:Config param scan_pipeline_iterations: True\n",
            "INFO:MaxText.pyconfig:Config param set_remat_policy_on_layers_per_stage: False\n",
            "INFO:MaxText.pyconfig:Config param set_remat_policy_on_pipeline_iterations: True\n",
            "INFO:MaxText.pyconfig:Config param sft_train_on_completion_only: True\n",
            "INFO:MaxText.pyconfig:Config param shard_mode: ShardMode.AUTO\n",
            "INFO:MaxText.pyconfig:Config param shard_optimizer_over_data: False\n",
            "INFO:MaxText.pyconfig:Config param sharding_strategy: None\n",
            "INFO:MaxText.pyconfig:Config param sharding_tolerance: 0.02\n",
            "INFO:MaxText.pyconfig:Config param shardy: True\n",
            "INFO:MaxText.pyconfig:Config param shared_experts: 1\n",
            "INFO:MaxText.pyconfig:Config param skip_first_n_steps_for_profiler: 1\n",
            "INFO:MaxText.pyconfig:Config param skip_jax_distributed_system: False\n",
            "INFO:MaxText.pyconfig:Config param sliding_window_size: 0\n",
            "INFO:MaxText.pyconfig:Config param solution_end_token: </answer>\n",
            "INFO:MaxText.pyconfig:Config param solution_start_token: <answer>\n",
            "INFO:MaxText.pyconfig:Config param source_checkpoint_layout: orbax\n",
            "INFO:MaxText.pyconfig:Config param sparse_matmul: True\n",
            "INFO:MaxText.pyconfig:Config param spatial_merge_size_for_vit: 2\n",
            "INFO:MaxText.pyconfig:Config param stack_prefill_result_cache: False\n",
            "INFO:MaxText.pyconfig:Config param stack_trace_interval_seconds: 600\n",
            "INFO:MaxText.pyconfig:Config param stack_trace_to_cloud: False\n",
            "INFO:MaxText.pyconfig:Config param step_deviation_interval_seconds: 30\n",
            "INFO:MaxText.pyconfig:Config param steps: 500\n",
            "INFO:MaxText.pyconfig:Config param subslice_shape: \n",
            "INFO:MaxText.pyconfig:Config param swap_space_vllm_gb: 2\n",
            "INFO:MaxText.pyconfig:Config param target_eval_loss: 0.0\n",
            "INFO:MaxText.pyconfig:Config param temperature_tuning: False\n",
            "INFO:MaxText.pyconfig:Config param temporal_patch_size_for_vit: 2\n",
            "INFO:MaxText.pyconfig:Config param tensorboard_dir: /tmp/maxtext_qwen06/2025-11-19-20-11-00/tensorboard/\n",
            "INFO:MaxText.pyconfig:Config param tensors_on_device: None\n",
            "INFO:MaxText.pyconfig:Config param tensors_to_offload: None\n",
            "INFO:MaxText.pyconfig:Config param tile_batch_seq: 512\n",
            "INFO:MaxText.pyconfig:Config param tile_embed_dim: 1024\n",
            "INFO:MaxText.pyconfig:Config param tile_fwd_batch_seq: 512\n",
            "INFO:MaxText.pyconfig:Config param tile_mlp_dim: 1024\n",
            "INFO:MaxText.pyconfig:Config param tile_size_for_vit: 336\n",
            "INFO:MaxText.pyconfig:Config param tokenize_eval_data: True\n",
            "INFO:MaxText.pyconfig:Config param tokenize_train_data: True\n",
            "INFO:MaxText.pyconfig:Config param tokenizer_path: Qwen/Qwen3-0.6B\n",
            "INFO:MaxText.pyconfig:Config param tokenizer_type: TokenizerType.HUGGINGFACE\n",
            "INFO:MaxText.pyconfig:Config param topk_routing_group: -1\n",
            "INFO:MaxText.pyconfig:Config param train_data_columns: ['question', 'answer']\n",
            "INFO:MaxText.pyconfig:Config param train_fraction: 1.0\n",
            "INFO:MaxText.pyconfig:Config param train_image_column: image\n",
            "INFO:MaxText.pyconfig:Config param train_split: train\n",
            "INFO:MaxText.pyconfig:Config param trainable_position_size: -1\n",
            "INFO:MaxText.pyconfig:Config param trainer_devices_fraction: 0.5\n",
            "INFO:MaxText.pyconfig:Config param upload_all_profiler_results: False\n",
            "INFO:MaxText.pyconfig:Config param use_batch_split_schedule: False\n",
            "INFO:MaxText.pyconfig:Config param use_chat_template: False\n",
            "INFO:MaxText.pyconfig:Config param use_chunked_prefill: False\n",
            "INFO:MaxText.pyconfig:Config param use_custom_sort_vjp: True\n",
            "INFO:MaxText.pyconfig:Config param use_dpo: False\n",
            "INFO:MaxText.pyconfig:Config param use_grpo: True\n",
            "INFO:MaxText.pyconfig:Config param use_iota_embed: False\n",
            "INFO:MaxText.pyconfig:Config param use_max_logit_estimate: -1\n",
            "INFO:MaxText.pyconfig:Config param use_multimodal: False\n",
            "INFO:MaxText.pyconfig:Config param use_pathways: True\n",
            "INFO:MaxText.pyconfig:Config param use_post_attn_norm: False\n",
            "INFO:MaxText.pyconfig:Config param use_post_ffw_norm: False\n",
            "INFO:MaxText.pyconfig:Config param use_qk_norm: True\n",
            "INFO:MaxText.pyconfig:Config param use_qk_norm_in_gdn: True\n",
            "INFO:MaxText.pyconfig:Config param use_qwix_quantization: False\n",
            "INFO:MaxText.pyconfig:Config param use_ragged_attention: False\n",
            "INFO:MaxText.pyconfig:Config param use_random_routing: False\n",
            "INFO:MaxText.pyconfig:Config param use_replicator_service: False\n",
            "INFO:MaxText.pyconfig:Config param use_ring_of_experts: False\n",
            "INFO:MaxText.pyconfig:Config param use_sft: True\n",
            "INFO:MaxText.pyconfig:Config param use_tokamax_gmm: False\n",
            "INFO:MaxText.pyconfig:Config param use_tokamax_splash: False\n",
            "INFO:MaxText.pyconfig:Config param use_truncation: True\n",
            "INFO:MaxText.pyconfig:Config param use_untrainable_positional_embedding: False\n",
            "INFO:MaxText.pyconfig:Config param use_vertex_tensorboard: False\n",
            "INFO:MaxText.pyconfig:Config param using_pipeline_parallelism: False\n",
            "INFO:MaxText.pyconfig:Config param v_head_dim: 128\n",
            "INFO:MaxText.pyconfig:Config param value_proj: RematLocation.REMAT\n",
            "INFO:MaxText.pyconfig:Config param vertex_tensorboard_project: \n",
            "INFO:MaxText.pyconfig:Config param vertex_tensorboard_region: \n",
            "INFO:MaxText.pyconfig:Config param vision_output_dim_for_vit: 4096\n",
            "INFO:MaxText.pyconfig:Config param vocab_size: 151936\n",
            "INFO:MaxText.pyconfig:Config param warmup_steps_fraction: 0.1\n",
            "INFO:MaxText.pyconfig:Config param weight_dtype: bfloat16\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "config = pyconfig.initialize(\n",
        "    [\n",
        "        \"\",\n",
        "        f\"{MAXTEXT_REPO_ROOT}/configs/sft.yml\",\n",
        "        f\"load_parameters_path={MODEL_CHECKPOINT_PATH}/0/items\",\n",
        "        f\"model_name={MODEL_NAME}\",\n",
        "        f\"hf_access_token={HF_TOKEN}\",\n",
        "        f\"base_output_directory={BASE_OUTPUT_DIRECTORY}\",\n",
        "        f\"run_name={RUN_NAME}\",\n",
        "        f\"tokenizer_path={TOKENIZER_PATH}\",\n",
        "        f\"hf_path={DATASET_NAME}\",\n",
        "        f\"train_split={TRAIN_DATA_SPLIT}\",\n",
        "        f\"hf_data_dir={HF_DATA_DIR}\",\n",
        "        f\"train_data_columns={TRAIN_DATA_COLUMNS}\",\n",
        "        \"steps=500\",\n",
        "        \"per_device_batch_size=1\",\n",
        "        \"max_target_length=1024\",\n",
        "        \"learning_rate=3e-6\",\n",
        "        \"weight_dtype=bfloat16\",\n",
        "        \"dtype=bfloat16\",\n",
        "        f\"chat_template_path={CHAT_TEMPLATE_PATH}\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9b0GWo-ZUQM"
      },
      "source": [
        "## Initial Setup & Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDqFmvUCZUQM"
      },
      "source": [
        "### Create Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "wscWYxrtZUQM",
        "outputId": "207c83e5-398b-4bc2-fd06-bbb67cf4f231",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 20 examples with a batch size of 1. This will result in 20 total batches for the test run.\n"
          ]
        }
      ],
      "source": [
        "test_dataset = get_test_dataset(config, tokenizer)\n",
        "test_dataset = test_dataset[:NUM_TEST_SAMPLES]\n",
        "test_dataset = test_dataset.to_iter_dataset().batch(BATCH_SIZE, drop_remainder=True)\n",
        "TOTAL_BATCHES = NUM_TEST_SAMPLES // BATCH_SIZE\n",
        "print(\n",
        "    f\"Processing {NUM_TEST_SAMPLES} examples with a batch size of {BATCH_SIZE}. This will result in {TOTAL_BATCHES} total batches for the test run.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLSvOOEUZUQM"
      },
      "source": [
        "### Create SFT Trainer State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "2IHsC0m6ZUQM",
        "outputId": "0b8c8109-13db-4831-ed64-817c74aa0005",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num_devices: 1, shape (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unknown model_mode: None",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4078937282.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msft_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_trainer_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/maxtext/src/MaxText/sft/sft_trainer.py\u001b[0m in \u001b[0;36msetup_trainer_state\u001b[0;34m(mt_config, goodput_recorder)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mmaybe_record_goodput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoodput_recorder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGoodputEvent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU_INIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_creation_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_nnx_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmt_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0mlearning_rate_schedule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxtext_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_learning_rate_schedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmt_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmt_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_schedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/maxtext/src/MaxText/model_creation_utils.py\u001b[0m in \u001b[0;36mcreate_nnx_model\u001b[0;34m(config, mesh, devices, model_mode, rng_key)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m   \u001b[0m_create_model_partial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmesh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmesh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m   \u001b[0mabstract_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_create_model_partial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mgraphdef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabstract_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabstract_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/flax/nnx/transforms/transforms.py\u001b[0m in \u001b[0;36meval_shape\u001b[0;34m(f, *args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_duplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_eval_shape_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/flax/nnx/transforms/transforms.py\u001b[0m in \u001b[0;36m_eval_shape_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_eval_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_duplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/maxtext/src/MaxText/model_creation_utils.py\u001b[0m in \u001b[0;36m_create_model\u001b[0;34m(mesh, model_mode, rng_key)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m       \u001b[0mrngs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRngs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng_key\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# disable dropout RNG for inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmesh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrngs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/maxtext/src/MaxText/model_creation_utils.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(config, devices, mesh, model_mode, rngs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mmesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMesh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevices_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmesh_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmesh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m   \u001b[0;31m# Return only the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/maxtext/src/MaxText/model_creation_utils.py\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(config, mesh, model_mode, rngs)\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0;31m# Model definition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m   \u001b[0mquant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantizations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_quantization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Mohit {model_mode}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_transformer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmesh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantizations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_quantize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/maxtext/src/MaxText/model_creation_utils.py\u001b[0m in \u001b[0;36mget_transformer_model\u001b[0;34m(config, mesh, quant, model_mode, rngs)\u001b[0m\n\u001b[1;32m    104\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrngs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmesh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrngs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_as_linen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmesh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/flax/nnx/pytreelib.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m   \u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m   \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mMutableArrayRepr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/flax/nnx/pytreelib.py\u001b[0m in \u001b[0;36m_graph_node_meta_call\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreprlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRepresentable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetaclass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPytreeMeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m   \u001b[0;34m\"\"\"Base class for all NNX objects.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/flax/nnx/pytreelib.py\u001b[0m in \u001b[0;36m_pytree_meta_construct\u001b[0;34m(cls, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mMutableArrayRepr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mMutableArrayRepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__nnx_repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/maxtext/src/MaxText/layers/models.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, mesh, quant, model_mode, rngs)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_seq_len_for_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m     \u001b[0mdummy_decoder_input_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0mdummy_decoder_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/maxtext/src/MaxText/max_utils.py\u001b[0m in \u001b[0;36mget_batch_seq_len_for_mode\u001b[0;34m(config, model_mode)\u001b[0m\n\u001b[1;32m    988\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m     \u001b[0;31m# Explicitly handle unknown modes instead of falling back to a default.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unknown model_mode: {model_mode}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown model_mode: None"
          ]
        }
      ],
      "source": [
        "trainer, mesh = sft_trainer.setup_trainer_state(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpKtEqzFZUQM"
      },
      "source": [
        "### Create vLLM Rollout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-pf_rbqZUQM"
      },
      "outputs": [],
      "source": [
        "tunix_model = TunixMaxTextAdapter(trainer.model)\n",
        "vllm_rollout = VllmRollout(\n",
        "    model=tunix_model,\n",
        "    tokenizer=tokenizer,\n",
        "    cache_config_or_size=1280,\n",
        "    mesh=mesh,\n",
        "    model_version=TOKENIZER_PATH,\n",
        "    hbm_utilization=0.8,\n",
        "    init_with_random_weights=True,\n",
        "    tpu_backend_type=\"jax\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "567gTxsEZUQM"
      },
      "source": [
        "## Evaluation before SFT Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnACa3zCZUQM"
      },
      "outputs": [],
      "source": [
        "print(\"Running Pre-SFT Evaluation...\")\n",
        "score = evaluate_model(test_dataset, vllm_rollout, debug=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5-M4iYkZUQN"
      },
      "outputs": [],
      "source": [
        "print(\"========================= Score for PRE-SFT Evaluation =========================\")\n",
        "print(f\"Percentage of test samples where the model produced the correct numerical answer: {score['correct']}%\")\n",
        "print(\n",
        "    f\"Percentage of test samples where the model produced the numerical answer within 10%: {score['partially_correct']}%\"\n",
        ")\n",
        "print(\n",
        "    f\"Percentage of test samples where the model's output adheres to the expected structure: {score['correct_format']}%\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJE1ookSAzz-"
      },
      "source": [
        "## SFT Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "mgwpNgQYCJEd",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(\"Starting SFT Training...\")\n",
        "trainer = sft_trainer.train_model(config, trainer, mesh)\n",
        "print(\"SFT Training Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEdNYRhwZUQN"
      },
      "source": [
        "## Evaluation after SFT Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcsZacZdZUQN"
      },
      "outputs": [],
      "source": [
        "print(\"Running Post-SFT Evaluation...\")\n",
        "model = TunixMaxTextAdapter(trainer.model)\n",
        "state = nnx.state(model)\n",
        "vllm_rollout.update_params(state)\n",
        "score = evaluate_model(test_dataset, vllm_rollout, debug=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "-JtYTPvJZUQN",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(\"========================= Score for POST-SFT Evaluation =========================\")\n",
        "print(f\"Percentage of test samples where the model produced the correct numerical answer: {score['correct']}%\")\n",
        "print(\n",
        "    f\"Percentage of test samples where the model produced the numerical answer within 10%: {score['partially_correct']}%\"\n",
        ")\n",
        "print(\n",
        "    f\"Percentage of test samples where the model's output adheres to the expected structure: {score['correct_format']}%\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}